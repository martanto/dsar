{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:32:01.682305Z",
     "start_time": "2024-04-04T09:32:01.676903Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from obspy import Stream\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.filesystem.sds import Client\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f14116e605c34d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initiate station variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "486d569993fb5b9c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:58:33.340014Z",
     "start_time": "2024-04-04T09:58:33.332784Z"
    }
   },
   "outputs": [],
   "source": [
    "network = \"VG\"\n",
    "station = \"PSAG\"\n",
    "location = \"00\"\n",
    "channel = \"EHZ\"\n",
    "\n",
    "nslc = \"{}.{}.{}.{}\".format(network, station, location, channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3a89592c1bdbe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initiate directories variables.   \n",
    "The `sds_directory` based on Seiscomp Data Structure (https://www.seiscomp.de/seiscomp3/doc/applications/slarchive/SDS.html)  \n",
    "The example of the SDS Directory can be found inside `input` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e965a885f8f139",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:29:16.338241Z",
     "start_time": "2024-04-04T09:29:16.333647Z"
    }
   },
   "outputs": [],
   "source": [
    "current_dir: str = os.getcwd()\n",
    "sds_directory: str = r\"D:\\Projects\\dsar\\input\"\n",
    "client = Client(sds_directory)\n",
    "\n",
    "output_directory: str = os.path.join(current_dir, \"output\")\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371a07385c2f5d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Add start_date and end_date parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a904db828f989873",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:29:17.243661Z",
     "start_time": "2024-04-04T09:29:17.235271Z"
    }
   },
   "outputs": [],
   "source": [
    "start_date: str = \"2017-12-01\"\n",
    "end_date: str = \"2017-12-03\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bands: dict[str, list[float]] = {\n",
    "    'HF' : [0.1, 8.0, 16.0],\n",
    "    'LF' : [0.1, 4.5, 8.0],\n",
    "}\n",
    "\n",
    "resample_rule: str = '10min'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:29:19.354674Z",
     "start_time": "2024-04-04T09:29:19.340111Z"
    }
   },
   "id": "15b8190370b1edb2",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "6e84a5fde0433a67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A method to generate list of date between two date periods. Returning `pd.DatetimeIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2f55a9383a957c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:29:20.147836Z",
     "start_time": "2024-04-04T09:29:20.135178Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dates(start: str, end: str) -> pd.DatetimeIndex:\n",
    "    return pd.date_range(start, end, freq=\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a3f41c7629181",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stream processing to get `dsar` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc3ed4606128c71",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:29:21.558722Z",
     "start_time": "2024-04-04T09:29:21.541323Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream_processing(\n",
    "        daily_mseed: Stream,\n",
    "        first_highpass: float = 0.1,\n",
    "        second_highpass: float = 8.0,\n",
    "        low_pass: float = 16.0\n",
    "    ) -> Stream:\n",
    "    stream = daily_mseed\n",
    "    stream.merge(fill_value=0)\n",
    "    stream.detrend('demean')\n",
    "    stream.filter('highpass', freq=first_highpass)\n",
    "    stream.integrate()\n",
    "    stream.filter('highpass', freq=second_highpass)\n",
    "    stream.filter('lowpass', freq=low_pass)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6272ffaa60f28",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Convert calculated `dsar` value into `pd.Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134a971b37e9cb63",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:29:23.138091Z",
     "start_time": "2024-04-04T09:29:23.131574Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_stream_to_series(stream: Stream) -> pd.Series:\n",
    "    index_time = pd.date_range(\n",
    "        start = stream[0].stats.starttime.datetime,\n",
    "        periods = stream[0].stats.npts,\n",
    "        freq = \"{}ms\".format(stream[0].stats.delta*1000)\n",
    "    )\n",
    "    \n",
    "    series = pd.Series(\n",
    "        data=np.abs(stream[0].data),\n",
    "        index=index_time,\n",
    "        name=stream[0].id,\n",
    "        dtype=stream[0].data.dtype)\n",
    "    \n",
    "    # print(series)\n",
    "    \n",
    "    return series.resample(resample_rule).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filling `streams` list variable"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfcd3b87a1ec6b6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fill_streams(date: UTCDateTime, band_values=None)-> Stream:\n",
    "    if band_values is None:\n",
    "        band_values = [0.1, 8.0, 16, 0]\n",
    "        \n",
    "    stream = client.get_waveforms(\n",
    "        network = network,\n",
    "        station = station,\n",
    "        location = location,\n",
    "        channel = channel,\n",
    "        starttime = date,\n",
    "        endtime= date + timedelta(days=1)\n",
    "    )\n",
    "    \n",
    "    # Check if stream is not empty (files not found)\n",
    "    # Return empty Stream if files are not found\n",
    "    if stream.count():\n",
    "        date_string = date.strftime('%Y-%m-%d')\n",
    "        print(\"⌚ Processing {} for {}\".format(date_string, stream[0].id))\n",
    "\n",
    "        # You can change the freq filter here\n",
    "        stream = stream_processing(\n",
    "            stream,\n",
    "            first_highpass = band_values[0],\n",
    "            second_highpass = band_values[1],\n",
    "            low_pass = band_values[2]\n",
    "        )\n",
    "        \n",
    "        return stream\n",
    "    else:\n",
    "        print(\"⚠️ {} :: File(s) not found!\".format(date.strftime('%Y-%m-%d')))\n",
    "        return Stream()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:29:24.610117Z",
     "start_time": "2024-04-04T09:29:24.597990Z"
    }
   },
   "id": "e965e75d99ddaa14",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filling `series` variable and save it to csv "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b78122f26ada4fa8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fill_series_and_save_to_csv(stream: Stream, band, band_values=None)-> pd.Series:\n",
    "    if band_values is None:\n",
    "        band_values = [0.1, 8.0, 16, 0]\n",
    "    date_string: str = stream[0].stats.starttime.datetime.strftime('%Y-%m-%d')\n",
    "    \n",
    "    filename: str = \"{}Hz_{}_{}\".format(\n",
    "        '-'.join(map(str,band_values)),\n",
    "        date_string,\n",
    "        stream[0].id\n",
    "    )\n",
    "    \n",
    "    csv_output = os.path.join(output_directory, band,\"{}.csv\".format(filename))\n",
    "    \n",
    "    print(\"↔️ Convert stream {} to series\".format(filename))\n",
    "    values = convert_stream_to_series(stream)\n",
    "    \n",
    "    print(\"💾 Saving to {}\".format(csv_output))\n",
    "    values.to_csv(os.path.join(output_directory,csv_output), header=False)\n",
    "    \n",
    "    return values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:29:26.407361Z",
     "start_time": "2024-04-04T09:29:26.400852Z"
    }
   },
   "id": "9f5e70eb79a0e445",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def concatenate_csv(band: str, station=None)-> str:\n",
    "    if station is None:\n",
    "        station = nslc\n",
    "        \n",
    "    df_list: list = []\n",
    "    \n",
    "    wildcard: str = '-'.join(map(str,bands[band]))\n",
    "    csv_files = glob.glob(os.path.join(\n",
    "        output_directory, band, \"{}Hz_*_{}.csv\".format(wildcard, station)))\n",
    "\n",
    "    for csv in csv_files:\n",
    "        df = pd.read_csv(csv, header=None)\n",
    "        df_list.append(df)\n",
    "        \n",
    "    big_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    combined_csv_files = os.path.join(output_directory, band,\"combined_{}Hz_{}.csv\".format(wildcard, station))\n",
    "    big_df.to_csv(\n",
    "        combined_csv_files,\n",
    "        index=False, header=False)\n",
    "    return combined_csv_files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:10:17.462020Z",
     "start_time": "2024-04-04T10:10:17.448644Z"
    }
   },
   "id": "ede3849cc2f943b2",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32884dd26aef9bec",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T09:29:28.983969Z",
     "start_time": "2024-04-04T09:29:28.972681Z"
    }
   },
   "outputs": [],
   "source": [
    "dates: list[UTCDateTime] = [UTCDateTime(date) for date in get_dates(start_date, end_date)]\n",
    "streams: dict[str, Stream] = {}\n",
    "series: dict[str, dict[str, pd.Series]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "🏃‍♀️ Using HF band with values [0.1, 8.0, 16.0]\n",
      "======================================\n",
      "⌚ Processing 2017-12-01 for VG.PSAG.00.EHZ\n",
      "↔️ Convert stream 0.1-8.0-16.0Hz_2017-12-01_VG.PSAG.00.EHZ to series\n",
      "💾 Saving to D:\\Projects\\dsar\\output\\HF\\0.1-8.0-16.0Hz_2017-12-01_VG.PSAG.00.EHZ.csv\n",
      "\n",
      "⌚ Processing 2017-12-02 for VG.PSAG.00.EHZ\n",
      "↔️ Convert stream 0.1-8.0-16.0Hz_2017-12-02_VG.PSAG.00.EHZ to series\n",
      "💾 Saving to D:\\Projects\\dsar\\output\\HF\\0.1-8.0-16.0Hz_2017-12-02_VG.PSAG.00.EHZ.csv\n",
      "\n",
      "⌚ Processing 2017-12-03 for VG.PSAG.00.EHZ\n",
      "↔️ Convert stream 0.1-8.0-16.0Hz_2017-12-03_VG.PSAG.00.EHZ to series\n",
      "💾 Saving to D:\\Projects\\dsar\\output\\HF\\0.1-8.0-16.0Hz_2017-12-03_VG.PSAG.00.EHZ.csv\n",
      "\n",
      "⌚ Combined CSV files saved into: D:\\Projects\\dsar\\output\\HF\\combined_0.1-8.0-16.0Hz_VG.PSAG.00.EHZ.csv\n",
      "\n",
      "=====================================\n",
      "🏃‍♀️ Using LF band with values [0.1, 4.5, 8.0]\n",
      "======================================\n",
      "⌚ Processing 2017-12-01 for VG.PSAG.00.EHZ\n",
      "↔️ Convert stream 0.1-4.5-8.0Hz_2017-12-01_VG.PSAG.00.EHZ to series\n",
      "💾 Saving to D:\\Projects\\dsar\\output\\LF\\0.1-4.5-8.0Hz_2017-12-01_VG.PSAG.00.EHZ.csv\n",
      "\n",
      "⌚ Processing 2017-12-02 for VG.PSAG.00.EHZ\n",
      "↔️ Convert stream 0.1-4.5-8.0Hz_2017-12-02_VG.PSAG.00.EHZ to series\n",
      "💾 Saving to D:\\Projects\\dsar\\output\\LF\\0.1-4.5-8.0Hz_2017-12-02_VG.PSAG.00.EHZ.csv\n",
      "\n",
      "⌚ Processing 2017-12-03 for VG.PSAG.00.EHZ\n",
      "↔️ Convert stream 0.1-4.5-8.0Hz_2017-12-03_VG.PSAG.00.EHZ to series\n",
      "💾 Saving to D:\\Projects\\dsar\\output\\LF\\0.1-4.5-8.0Hz_2017-12-03_VG.PSAG.00.EHZ.csv\n",
      "\n",
      "⌚ Combined CSV files saved into: D:\\Projects\\dsar\\output\\LF\\combined_0.1-4.5-8.0Hz_VG.PSAG.00.EHZ.csv\n",
      "\n",
      "✅Finish!\n"
     ]
    }
   ],
   "source": [
    "# We can optimize this using parallel computation\n",
    "for band in bands.keys():\n",
    "    # Create output directory per band\n",
    "    os.makedirs(os.path.join(output_directory, band), exist_ok=True)\n",
    "    \n",
    "    # Get band values\n",
    "    band_values: list[float] = bands[band]\n",
    "    \n",
    "    # Initiate series per band with empty dict\n",
    "    series[band]: dict[str, pd.Series] = {}\n",
    "    print(\"=====================================\")\n",
    "    print(\"🏃‍♀️ Using {} band with values {}\".format(band, band_values))\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    # Looping through date\n",
    "    for date in dates:\n",
    "        date_string = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Add stream value to streams variable\n",
    "        streams[date_string]: Stream = fill_streams(date, band_values)\n",
    "        \n",
    "        # Check if stream per date is empty or not\n",
    "        # Skip converting if data is not found\n",
    "        if streams[date_string].count():\n",
    "            # Converting stream value to series and save it as CSV\n",
    "            series[band][date_string]: pd.Series = fill_series_and_save_to_csv(streams[date_string], band, band_values)\n",
    "    \n",
    "    # Combining CSV files\n",
    "    combined_csv_file = concatenate_csv(band)\n",
    "    print(\"⌚ Combined CSV files saved into: {}\".format(combined_csv_file))\n",
    "    print(\"\")\n",
    "print(\"✅Finish!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:11:06.193966Z",
     "start_time": "2024-04-04T10:11:02.761279Z"
    }
   },
   "id": "1fd9aeea1b12b97e",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef93a2819b0a811",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series.keys()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "series['LF'].keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67bfbcd88a8f769",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "series['HF']['2017-12-01']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "614421545c7d522f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eed1167b340ea7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series['HF']['2017-12-01'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403dd15-3a07-4fa7-90c7-f08c30ac5058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
