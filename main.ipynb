{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.583230Z",
     "start_time": "2024-04-05T22:01:43.569509Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from obspy import Stream\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.filesystem.sds import Client\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f14116e605c34d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initiate station variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486d569993fb5b9c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.599302Z",
     "start_time": "2024-04-05T22:01:43.591957Z"
    }
   },
   "outputs": [],
   "source": [
    "network = \"VG\"\n",
    "station = \"ABNG\"\n",
    "location = \"00\"\n",
    "channel = \"EHZ\"\n",
    "\n",
    "nslc = \"{}.{}.{}.{}\".format(network, station, location, channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3a89592c1bdbe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initiate directories variables.   \n",
    "The `sds_directory` based on Seiscomp Data Structure (https://www.seiscomp.de/seiscomp3/doc/applications/slarchive/SDS.html)  \n",
    "The example of the SDS Directory can be found inside `input` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79e965a885f8f139",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.615242Z",
     "start_time": "2024-04-05T22:01:43.600386Z"
    }
   },
   "outputs": [],
   "source": [
    "current_dir: str = os.getcwd()\n",
    "sds_directory: str = r\"D:\\Data\\SDS\"\n",
    "client = Client(sds_directory)\n",
    "\n",
    "output_directory: str = os.path.join(current_dir, \"output\")\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371a07385c2f5d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Add start_date and end_date parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a904db828f989873",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.631243Z",
     "start_time": "2024-04-05T22:01:43.616310Z"
    }
   },
   "outputs": [],
   "source": [
    "start_date: str = \"2017-10-01\"\n",
    "end_date: str = \"2018-07-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b8190370b1edb2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.646622Z",
     "start_time": "2024-04-05T22:01:43.632358Z"
    }
   },
   "outputs": [],
   "source": [
    "bands: dict[str, list[float]] = {\n",
    "    'HF' : [0.1, 8.0, 16.0],\n",
    "    'LF' : [0.1, 4.5, 8.0],\n",
    "}\n",
    "\n",
    "resample_rule: str = '10min'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e84a5fde0433a67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A method to generate list of date between two date periods. Returning `pd.DatetimeIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb2f55a9383a957c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.662315Z",
     "start_time": "2024-04-05T22:01:43.647745Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dates(start: str, end: str) -> pd.DatetimeIndex:\n",
    "    return pd.date_range(start, end, freq=\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a3f41c7629181",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stream processing to get `dsar` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dc3ed4606128c71",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.678346Z",
     "start_time": "2024-04-05T22:01:43.663389Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream_processing(\n",
    "        daily_mseed: Stream,\n",
    "        first_highpass: float = 0.1,\n",
    "        second_highpass: float = 8.0,\n",
    "        low_pass: float = 16.0\n",
    "    ) -> Stream:\n",
    "    stream = daily_mseed\n",
    "    stream.merge(fill_value=0)\n",
    "    stream.detrend('demean')\n",
    "    stream.filter('highpass', freq=first_highpass)\n",
    "    stream.integrate()\n",
    "    stream.filter('highpass', freq=second_highpass)\n",
    "    stream.filter('lowpass', freq=low_pass)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6272ffaa60f28",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Convert calculated `dsar` value into `pd.Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "134a971b37e9cb63",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.693488Z",
     "start_time": "2024-04-05T22:01:43.679569Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_stream_to_series(stream: Stream) -> pd.Series:\n",
    "    index_time = pd.date_range(\n",
    "        start = stream[0].stats.starttime.datetime,\n",
    "        periods = stream[0].stats.npts,\n",
    "        freq = \"{}ms\".format(stream[0].stats.delta*1000)\n",
    "    )\n",
    "    \n",
    "    _series = pd.Series(\n",
    "        data=np.abs(stream[0].data),\n",
    "        index=index_time,\n",
    "        name=stream[0].id,\n",
    "        dtype=stream[0].data.dtype)\n",
    "    \n",
    "    return _series.resample(resample_rule).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd3b87a1ec6b6f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Filling `streams` list variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e965e75d99ddaa14",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.708715Z",
     "start_time": "2024-04-05T22:01:43.694529Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_streams(date: UTCDateTime, band_values=None)-> Stream:\n",
    "    if band_values is None:\n",
    "        band_values = [0.1, 8.0, 16, 0]\n",
    "        \n",
    "    stream = client.get_waveforms(\n",
    "        network = network,\n",
    "        station = station,\n",
    "        location = location,\n",
    "        channel = channel,\n",
    "        starttime = date,\n",
    "        endtime= date + timedelta(days=1)\n",
    "    )\n",
    "    \n",
    "    # Check if stream is not empty (files not found)\n",
    "    # Return empty Stream if files are not found\n",
    "    if stream.count():\n",
    "        date_string = date.strftime('%Y-%m-%d')\n",
    "        print(\"⌚ Processing {} for {}\".format(date_string, stream[0].id))\n",
    "\n",
    "        # You can change the freq filter here\n",
    "        stream = stream_processing(\n",
    "            stream,\n",
    "            first_highpass = band_values[0],\n",
    "            second_highpass = band_values[1],\n",
    "            low_pass = band_values[2]\n",
    "        )\n",
    "        \n",
    "        return stream\n",
    "    else:\n",
    "        print(\"⚠️ {} :: File(s) not found!\".format(date.strftime('%Y-%m-%d')))\n",
    "        return Stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78122f26ada4fa8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Filling `series` variable and save it to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f5e70eb79a0e445",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.723936Z",
     "start_time": "2024-04-05T22:01:43.710985Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_series_and_save_to_csv(stream: Stream, band, band_values=None)-> pd.Series:\n",
    "    if band_values is None:\n",
    "        band_values = [0.1, 8.0, 16, 0]\n",
    "    date_string: str = stream[0].stats.starttime.datetime.strftime('%Y-%m-%d')\n",
    "    \n",
    "    filename: str = \"{}Hz_{}_{}\".format(\n",
    "        '-'.join(map(str,band_values)),\n",
    "        date_string,\n",
    "        stream[0].id\n",
    "    )\n",
    "    \n",
    "    csv_output = os.path.join(output_directory, band,\"{}.csv\".format(filename))\n",
    "    \n",
    "    print(\"↔️ Convert stream {} to series\".format(filename))\n",
    "    values = convert_stream_to_series(stream)\n",
    "    \n",
    "    print(\"💾 Saving to {}\".format(csv_output))\n",
    "    values.to_csv(os.path.join(output_directory,csv_output), header=False)\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf281ea66334041",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Combining CSV files per band and station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ede3849cc2f943b2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.740105Z",
     "start_time": "2024-04-05T22:01:43.724981Z"
    }
   },
   "outputs": [],
   "source": [
    "def concatenate_csv(band: str, station=None)-> str:\n",
    "    if station is None:\n",
    "        station = nslc\n",
    "        \n",
    "    df_list: list = []\n",
    "    \n",
    "    wildcard: str = '-'.join(map(str,bands[band]))\n",
    "    csv_files: list[str] = glob.glob(os.path.join(\n",
    "        output_directory, band, \"{}Hz_*_{}.csv\".format(wildcard, station)))\n",
    "\n",
    "    for csv in csv_files:\n",
    "        df = pd.read_csv(csv, header=None)\n",
    "        df_list.append(df)\n",
    "        \n",
    "    big_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    combined_csv_files: str = os.path.join(\n",
    "        output_directory, band,\"combined_{}Hz_{}.csv\".format(wildcard, station))\n",
    "    \n",
    "    big_df.to_csv(\n",
    "        combined_csv_files,\n",
    "        index=False, header=False)\n",
    "    return combined_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32884dd26aef9bec",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:43.755880Z",
     "start_time": "2024-04-05T22:01:43.742200Z"
    }
   },
   "outputs": [],
   "source": [
    "dates: list[UTCDateTime] = [UTCDateTime(date) for date in get_dates(start_date, end_date)]\n",
    "streams: dict[str, Stream] = {}\n",
    "series: dict[str, dict[str, pd.Series]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd9aeea1b12b97e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T22:01:44.741551Z",
     "start_time": "2024-04-05T22:01:43.756936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "🏃‍♀️ Using HF band with values [0.1, 8.0, 16.0]\n",
      "======================================\n",
      "⚠️ 2017-10-01 :: File(s) not found!\n",
      "⚠️ 2017-10-02 :: File(s) not found!\n",
      "⚠️ 2017-10-03 :: File(s) not found!\n",
      "⚠️ 2017-10-04 :: File(s) not found!\n",
      "⚠️ 2017-10-05 :: File(s) not found!\n",
      "⚠️ 2017-10-06 :: File(s) not found!\n",
      "⚠️ 2017-10-07 :: File(s) not found!\n",
      "⚠️ 2017-10-08 :: File(s) not found!\n",
      "⚠️ 2017-10-09 :: File(s) not found!\n",
      "⚠️ 2017-10-10 :: File(s) not found!\n",
      "⚠️ 2017-10-11 :: File(s) not found!\n",
      "⚠️ 2017-10-12 :: File(s) not found!\n",
      "⚠️ 2017-10-13 :: File(s) not found!\n",
      "⚠️ 2017-10-14 :: File(s) not found!\n",
      "⚠️ 2017-10-15 :: File(s) not found!\n",
      "⚠️ 2017-10-16 :: File(s) not found!\n",
      "⚠️ 2017-10-17 :: File(s) not found!\n",
      "⌚ Processing 2017-10-18 for VG.ABNG.00.EHZ\n",
      "↔️ Convert stream 0.1-8.0-16.0Hz_2017-10-18_VG.ABNG.00.EHZ to series\n",
      "💾 Saving to D:\\Projects\\dsar\\output\\HF\\0.1-8.0-16.0Hz_2017-10-18_VG.ABNG.00.EHZ.csv\n",
      "⌚ Processing 2017-10-19 for VG.ABNG.00.EHZ\n",
      "↔️ Convert stream 0.1-8.0-16.0Hz_2017-10-19_VG.ABNG.00.EHZ to series\n",
      "💾 Saving to D:\\Projects\\dsar\\output\\HF\\0.1-8.0-16.0Hz_2017-10-19_VG.ABNG.00.EHZ.csv\n",
      "⌚ Processing 2017-10-20 for VG.ABNG.00.EHZ\n",
      "↔️ Convert stream 0.1-8.0-16.0Hz_2017-10-20_VG.ABNG.00.EHZ to series\n",
      "💾 Saving to D:\\Projects\\dsar\\output\\HF\\0.1-8.0-16.0Hz_2017-10-20_VG.ABNG.00.EHZ.csv\n",
      "⌚ Processing 2017-10-21 for VG.ABNG.00.EHZ\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Can't merge traces with same ids but differing data types!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 20\u001B[0m\n\u001B[0;32m     17\u001B[0m date_string \u001B[38;5;241m=\u001B[39m date\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Add stream value to streams variable\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m streams[date_string]: Stream \u001B[38;5;241m=\u001B[39m \u001B[43mfill_streams\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mband_values\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Check if stream per date is empty or not\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Skip converting if data is not found\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streams[date_string]\u001B[38;5;241m.\u001B[39mcount():\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;66;03m# Converting stream value to series and save it as CSV\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[22], line 21\u001B[0m, in \u001B[0;36mfill_streams\u001B[1;34m(date, band_values)\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⌚ Processing \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(date_string, stream[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mid))\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# You can change the freq filter here\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m     stream \u001B[38;5;241m=\u001B[39m \u001B[43mstream_processing\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfirst_highpass\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mband_values\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[43msecond_highpass\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mband_values\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_pass\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mband_values\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m stream\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Cell \u001B[1;32mIn[20], line 8\u001B[0m, in \u001B[0;36mstream_processing\u001B[1;34m(daily_mseed, first_highpass, second_highpass, low_pass)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstream_processing\u001B[39m(\n\u001B[0;32m      2\u001B[0m         daily_mseed: Stream,\n\u001B[0;32m      3\u001B[0m         first_highpass: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.1\u001B[39m,\n\u001B[0;32m      4\u001B[0m         second_highpass: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8.0\u001B[39m,\n\u001B[0;32m      5\u001B[0m         low_pass: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16.0\u001B[39m\n\u001B[0;32m      6\u001B[0m     ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Stream:\n\u001B[0;32m      7\u001B[0m     stream \u001B[38;5;241m=\u001B[39m daily_mseed\n\u001B[1;32m----> 8\u001B[0m     \u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     stream\u001B[38;5;241m.\u001B[39mdetrend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdemean\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     10\u001B[0m     stream\u001B[38;5;241m.\u001B[39mfilter(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhighpass\u001B[39m\u001B[38;5;124m'\u001B[39m, freq\u001B[38;5;241m=\u001B[39mfirst_highpass)\n",
      "File \u001B[1;32mD:\\Projects\\miniconda\\envs\\dsar\\lib\\site-packages\\obspy\\core\\stream.py:2037\u001B[0m, in \u001B[0;36mStream.merge\u001B[1;34m(self, method, fill_value, interpolation_samples, **kwargs)\u001B[0m\n\u001B[0;32m   2035\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[0;32m   2036\u001B[0m \u001B[38;5;66;03m# check sampling rates and dtypes\u001B[39;00m\n\u001B[1;32m-> 2037\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_merge_checks\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2038\u001B[0m \u001B[38;5;66;03m# remember order of traces\u001B[39;00m\n\u001B[0;32m   2039\u001B[0m order \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mid\u001B[39m(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraces]\n",
      "File \u001B[1;32mD:\\Projects\\miniconda\\envs\\dsar\\lib\\site-packages\\obspy\\core\\stream.py:1981\u001B[0m, in \u001B[0;36mStream._merge_checks\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1978\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trace\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m dtype[trace\u001B[38;5;241m.\u001B[39mid]:\n\u001B[0;32m   1979\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt merge traces with same ids but differing \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[0;32m   1980\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata types!\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1981\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(msg)\n\u001B[0;32m   1982\u001B[0m \u001B[38;5;66;03m# Check calibration factor.\u001B[39;00m\n\u001B[0;32m   1983\u001B[0m calib\u001B[38;5;241m.\u001B[39msetdefault(trace\u001B[38;5;241m.\u001B[39mid, trace\u001B[38;5;241m.\u001B[39mstats\u001B[38;5;241m.\u001B[39mcalib)\n",
      "\u001B[1;31mException\u001B[0m: Can't merge traces with same ids but differing data types!"
     ]
    }
   ],
   "source": [
    "# We can optimize this using parallel computation\n",
    "for band in bands.keys():\n",
    "    # Create output directory per band\n",
    "    os.makedirs(os.path.join(output_directory, band), exist_ok=True)\n",
    "    \n",
    "    # Get band values\n",
    "    band_values: list[float] = bands[band]\n",
    "    \n",
    "    # Initiate series per band with empty dict\n",
    "    series[band]: dict[str, pd.Series] = {}\n",
    "    print(\"=====================================\")\n",
    "    print(\"🏃‍♀️ Using {} band with values {}\".format(band, band_values))\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    # Looping through date\n",
    "    for date in dates:\n",
    "        date_string = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Add stream value to streams variable\n",
    "        streams[date_string]: Stream = fill_streams(date, band_values)\n",
    "        \n",
    "        # Check if stream per date is empty or not\n",
    "        # Skip converting if data is not found\n",
    "        if streams[date_string].count():\n",
    "            # Converting stream value to series and save it as CSV\n",
    "            series[band][date_string]: pd.Series = fill_series_and_save_to_csv(streams[date_string], band, band_values)\n",
    "    \n",
    "    # Combining CSV files\n",
    "    combined_csv_file = concatenate_csv(band)\n",
    "    print(\"⌚ Combined CSV files saved into: {}\".format(combined_csv_file))\n",
    "    print(\"\")\n",
    "print(\"✅Finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef93a2819b0a811",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T12:09:47.693561Z",
     "start_time": "2024-04-04T12:09:47.680635Z"
    }
   },
   "outputs": [],
   "source": [
    "# series.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67bfbcd88a8f769",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T12:09:47.708823Z",
     "start_time": "2024-04-04T12:09:47.695890Z"
    }
   },
   "outputs": [],
   "source": [
    "# series['LF'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "614421545c7d522f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T12:09:47.720302Z",
     "start_time": "2024-04-04T12:09:47.710265Z"
    }
   },
   "outputs": [],
   "source": [
    "# series['HF']['2017-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9eed1167b340ea7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T12:09:47.735400Z",
     "start_time": "2024-04-04T12:09:47.721585Z"
    }
   },
   "outputs": [],
   "source": [
    "# series['HF']['2017-12-01'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2403dd15-3a07-4fa7-90c7-f08c30ac5058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T12:09:47.750361Z",
     "start_time": "2024-04-04T12:09:47.736615Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
